{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dde09bb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# --- [ìˆ˜ì •] ê²½ë¡œ ì„¤ì • ë° import ---\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# í˜„ìž¬ íŒŒì¼(train.py)ì˜ ë””ë ‰í† ë¦¬ ( .../project_root/models )\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m SCRIPT_DIR = os.path.dirname(os.path.abspath(\u001b[34;43m__file__\u001b[39;49m))\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# ( .../project_root )\u001b[39;00m\n\u001b[32m     16\u001b[39m PROJECT_ROOT = os.path.dirname(SCRIPT_DIR) \n",
      "\u001b[31mNameError\u001b[39m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# models/train.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# --- [ìˆ˜ì •] ê²½ë¡œ ì„¤ì • ë° import ---\n",
    "# í˜„ìž¬ íŒŒì¼(train.py)ì˜ ë””ë ‰í† ë¦¬ ( .../project_root/models )\n",
    "SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# ( .../project_root )\n",
    "PROJECT_ROOT = os.path.dirname(SCRIPT_DIR) \n",
    "\n",
    "# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€í•˜ì—¬ ëª¨ë“ˆ ìž„í¬íŠ¸ê°€ ê°€ëŠ¥í•˜ë„ë¡ í•¨\n",
    "# (ì´ë¯¸ modelsê°€ PYTHONPATHì— ìž¡í˜€ìžˆë‹¤ë©´ í•„ìš” ì—†ì„ ìˆ˜ ìžˆìŒ)\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "# ë°ì´í„° ê²½ë¡œëŠ” í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ 'data' í´ë”\n",
    "DATA_ROOT = os.path.join(PROJECT_ROOT, \"data\") \n",
    "# ëª¨ë¸ ì €ìž¥ ê²½ë¡œëŠ” í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— ì €ìž¥\n",
    "MODEL_SAVE_PATH = os.path.join(PROJECT_ROOT, \"cnn_fft_detector.pth\")\n",
    "\n",
    "try:\n",
    "    # models/preprocessing/preprocessing.pyì—ì„œ FaceData ìž„í¬íŠ¸\n",
    "    from models.preprocessing.preprocessing import FaceData \n",
    "    # models/base_models.pyì—ì„œ CNNClfWithFFT ìž„í¬íŠ¸\n",
    "    from models.base_models import CNNClfWithFFT\n",
    "except ImportError as e:\n",
    "    print(f\"Error: ëª¨ë“ˆ ìž„í¬íŠ¸ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”. (Error: {e})\")\n",
    "    print(f\"SCRIPT_DIR: {SCRIPT_DIR}\")\n",
    "    print(f\"PROJECT_ROOT: {PROJECT_ROOT}\")\n",
    "    print(f\"sys.path: {sys.path}\")\n",
    "    exit()\n",
    "# --- [ìˆ˜ì • ë] ---\n",
    "\n",
    "\n",
    "# --- í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ---\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 20\n",
    "TEST_SPLIT_RATIO = 0.2\n",
    "NUM_WORKERS = 4 \n",
    "providers = [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"] \n",
    "det_size = (640, 640) \n",
    "\n",
    "# (collate_fn_safe í•¨ìˆ˜ëŠ” ì´ì „ê³¼ ë™ì¼)\n",
    "def collate_fn_safe(batch):\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    if not batch:\n",
    "        return torch.empty(0), torch.empty(0)\n",
    "    try:\n",
    "        return torch.utils.data.dataloader.default_collate(batch)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Warning: Skipping batch due to error: {e}\")\n",
    "        return torch.empty(0), torch.empty(0)\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"Loading data from: {DATA_ROOT}\")\n",
    "\n",
    "    # --- 1. ë°ì´í„°ì…‹ ë¡œë“œ ë° ë¶„ë¦¬ ---\n",
    "    print(\"Loading dataset...\")\n",
    "    try:\n",
    "        full_dataset = FaceData(\n",
    "            root=DATA_ROOT,\n",
    "            normalize=True,\n",
    "            providers=providers,\n",
    "            det_size=det_size\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        print(f\"'{DATA_ROOT}' ê²½ë¡œì— 'real' ë° 'ai_images' í´ë”ê°€ ìžˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "        return\n",
    "\n",
    "    if len(full_dataset) == 0:\n",
    "        print(\"Error: No data found. Check DATA_ROOT.\")\n",
    "        return\n",
    "        \n",
    "    indices = list(range(len(full_dataset)))\n",
    "    labels = [s[1] for s in full_dataset.samples]\n",
    "\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        indices,\n",
    "        test_size=TEST_SPLIT_RATIO,\n",
    "        stratify=labels,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        full_dataset, batch_size=BATCH_SIZE, sampler=train_sampler,\n",
    "        num_workers=NUM_WORKERS, pin_memory=True, collate_fn=collate_fn_safe\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        full_dataset, batch_size=BATCH_SIZE, sampler=val_sampler,\n",
    "        num_workers=NUM_WORKERS, pin_memory=True, collate_fn=collate_fn_safe\n",
    "    )\n",
    "    \n",
    "    print(f\"Total samples: {len(full_dataset)}\")\n",
    "    print(f\"Training samples: {len(train_indices)}\")\n",
    "    print(f\"Validation samples: {len(val_indices)}\")\n",
    "\n",
    "    # --- 2. ëª¨ë¸, ì†ì‹¤ í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì € ì •ì˜ ---\n",
    "    model = CNNClfWithFFT(num_classes=2).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # --- 3. í•™ìŠµ ë£¨í”„ ---\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{NUM_EPOCHS} ---\")\n",
    "        \n",
    "        # --- í•™ìŠµ (Train) ---\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_preds = 0\n",
    "        total_preds = 0\n",
    "\n",
    "        pbar_train = tqdm(train_loader, desc=\"Train\", unit=\"batch\", leave=False)\n",
    "        for inputs, labels in pbar_train:\n",
    "            if inputs.nelement() == 0: continue \n",
    "\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_preds += torch.sum(preds == labels.data)\n",
    "            total_preds += inputs.size(0)\n",
    "            \n",
    "            pbar_train.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "        \n",
    "        if total_preds > 0:\n",
    "            epoch_loss = running_loss / total_preds\n",
    "            epoch_acc = correct_preds.double() / total_preds\n",
    "            print(f\"Train Loss: {epoch_loss:.4f} | Train Acc: {epoch_acc:.4f}\")\n",
    "        else:\n",
    "            print(\"Warning: No training data processed in this epoch.\")\n",
    "\n",
    "        # --- ê²€ì¦ (Validation) ---\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pbar_val = tqdm(val_loader, desc=\"Val\", unit=\"batch\", leave=False)\n",
    "            for inputs, labels in pbar_val:\n",
    "                if inputs.nelement() == 0: continue\n",
    "\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_corrects += torch.sum(preds == labels.data)\n",
    "                val_total += inputs.size(0)\n",
    "\n",
    "        if val_total > 0:\n",
    "            val_epoch_loss = val_loss / val_total\n",
    "            val_epoch_acc = val_corrects.double() / val_total\n",
    "            print(f\"Val Loss:   {val_epoch_loss:.4f} | Val Acc:   {val_epoch_acc:.4f}\")\n",
    "\n",
    "            if val_epoch_acc > best_val_acc:\n",
    "                print(f\"ðŸš€ New best model found! Saving to {MODEL_SAVE_PATH}\")\n",
    "                best_val_acc = val_epoch_acc\n",
    "                torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "        else:\n",
    "            print(\"Warning: No validation data processed in this epoch.\")\n",
    "\n",
    "    print(\"\\nâœ… Training finished.\")\n",
    "    print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
