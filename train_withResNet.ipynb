{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c836810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score\n",
    "from typing import Tuple, Sequence, Optional\n",
    "\n",
    "from models.Model_withResNet import DeepclfwithResNet\n",
    "\n",
    "IMAGE_EXTS = {\".jpg\", \".jpeg\", \".png\"}\n",
    "VIDEO_EXTS = {\".avi\", \".mp4\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8811df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device  = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9051933",
   "metadata": {},
   "source": [
    "### ì–¼êµ´ íƒì§€ í•¨ìˆ˜ ì„ ì–¸\n",
    "- input : Image.Image íƒ€ì…ì˜ single image\n",
    "- output : target_sizeì˜ ì–¼êµ´ ì¤‘ì‹¬ìœ¼ë¡œ í¬ë¡­ëœ Image.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d751eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_crop_face_optimized(\n",
    "    image: Image.Image,\n",
    "    target_size: Tuple[int, int] = (224, 224),\n",
    "    providers: Sequence[str] = (\"CUDAExecutionProvider\", \"CPUExecutionProvider\"),\n",
    "    model: str = \"buffalo_s\"\n",
    ") -> Optional[Image.Image]:\n",
    "    \"\"\"\n",
    "    ì…ë ¥: PIL.Image (ì„ì˜ ëª¨ë“œ)\n",
    "    ì¶œë ¥: PIL.Image (RGB, 224x224)\n",
    "    ì²˜ë¦¬:\n",
    "      1 ì–¼êµ´ ì°¾ì„ ë•Œë§Œ ì›ë³¸ì„ ê¸´ ë³€ ê¸°ì¤€ìœ¼ë¡œ 512ë¡œ ë¦¬ì‚¬ì´ì¦ˆí•´ì„œ detect\n",
    "      2 detectëœ ì½” ì¢Œí‘œë¥¼ ì›ë³¸ ì¢Œí‘œë¡œ ë˜ëŒë¦¼ (scale ì—­ì‚°)\n",
    "      3 ì›ë³¸ì—ì„œ ì¤‘ì‹¬ ê¸°ì¤€ 512x512 crop (ì‘ì€ ì´ë¯¸ì§€ëŠ” 256 ë¦¬ì‚¬ì´ì¦ˆ í›„ ì‚¬ìš©)\n",
    "      4 512â†’256ìœ¼ë¡œ ì¶•ì†Œ (ì‘ì€ ì´ë¯¸ì§€ëŠ” ì´ë¯¸ 256)\n",
    "      5 256 ê°€ìš´ë° 224Ã—224 center crop\n",
    "      6 í•­ìƒ RGB PIL.Imageë¡œ ë°˜í™˜\n",
    "    \"\"\"\n",
    "\n",
    "    if image is None:\n",
    "        return None\n",
    "\n",
    "    # (0) ëª¨ë“  ì…ë ¥ ì´ë¯¸ì§€ë¥¼ RGBë¡œ í†µì¼\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "\n",
    "    # PIL â†’ numpy (RGB â†’ BGR)\n",
    "    img_np = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "    H, W = img_np.shape[:2]\n",
    "\n",
    "    # (1) ì–¼êµ´ ê²€ì¶œê¸° ì¤€ë¹„\n",
    "    app = FaceAnalysis(name=model, providers=list(providers))\n",
    "    app.prepare(ctx_id=0)\n",
    "\n",
    "    # (2) íƒì§€ë¥¼ ìœ„í•´ ê¸´ ë³€ ê¸°ì¤€ 512ë¡œ ì¶•ì†Œ\n",
    "    detect_side = 512\n",
    "    scale = detect_side / max(H, W)\n",
    "    det_w = int(W * scale)\n",
    "    det_h = int(H * scale)\n",
    "    det_img = cv2.resize(img_np, (det_w, det_h))\n",
    "\n",
    "    # (3) ì–¼êµ´ íƒì§€\n",
    "    faces = app.get(det_img)\n",
    "    if not faces:\n",
    "        cx, cy = W // 2, H // 4\n",
    "    else:\n",
    "        f = max(faces, key=lambda x: (x.bbox[2]-x.bbox[0])*(x.bbox[3]-x.bbox[1]))\n",
    "        cx, cy = f.kps[2]\n",
    "        cx, cy = int(cx / scale), int(cy / scale)\n",
    "\n",
    "    # (4) í¬ë¡­ í¬ê¸° ê²°ì •\n",
    "    crop_size = 512 if (H >= 512 and W >= 512) else 256\n",
    "    half = crop_size // 2\n",
    "    x1 = min(max(cx - half, 0), max(W - crop_size, 0))\n",
    "    y1 = min(max(cy - half, 0), max(H - crop_size, 0))\n",
    "    cropped = img_np[y1:y1 + crop_size, x1:x1 + crop_size]\n",
    "    if cropped is None or cropped.size == 0:\n",
    "        return None\n",
    "\n",
    "    # (5) 512ì¼ ê²½ìš° 256ìœ¼ë¡œ ì¶•ì†Œ, ì‘ì€ ì´ë¯¸ì§€ëŠ” ê·¸ëŒ€ë¡œ 256\n",
    "    resized_256 = cv2.resize(cropped, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # (6) ì¤‘ì•™ 224Ã—224 crop\n",
    "    start = (256 - 224) // 2\n",
    "    end = start + 224\n",
    "    face_np = resized_256[start:end, start:end]\n",
    "\n",
    "    # (7) numpy â†’ PIL, í•­ìƒ RGBë¡œ ë³´ì¥\n",
    "    face_rgb = cv2.cvtColor(face_np, cv2.COLOR_BGR2RGB)\n",
    "    face_img = Image.fromarray(face_rgb).convert(\"RGB\")\n",
    "\n",
    "    return face_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717e8e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ì „ì²˜ë¦¬í•œ ì´ë¯¸ì§€ ì €ì¥ -> í•™ìŠµì‹œë§ˆë‹¤ ì „ì²˜ë¦¬ë¥¼ ë‹¤ì‹œ ìˆ˜í–‰í•  í•„ìš” ì—†ìŒ\n",
    "\n",
    "def preprocess_and_save_dataset(\n",
    "        input_root: str, \n",
    "        output_root: str, \n",
    "        center_frame_only: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    íŠ¹ì • í´ë” ë‚´ì˜ ì´ë¯¸ì§€ ë° ë¹„ë””ì˜¤ì—ì„œ ì–¼êµ´ì„ ê²€ì¶œ/í¬ë¡­í•˜ì—¬ ë‹¤ë¥¸ í´ë”ì— ì €ì¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "    Args:\n",
    "        input_root (str): ì›ë³¸ ë°ì´í„°ì…‹ ë£¨íŠ¸ í´ë” (real/fake í¬í•¨).\n",
    "        output_root (str): ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ë¥¼ ì €ì¥í•  ëŒ€ìƒ ë£¨íŠ¸ í´ë”.\n",
    "        center_frame_only (bool): ë¹„ë””ì˜¤ì—ì„œ ì¤‘ì•™ í”„ë ˆì„ë§Œ ì¶”ì¶œí• ì§€ ì—¬ë¶€.\n",
    "                                  (False ì‹œ, ë¹„ë””ì˜¤ ì „ì²´ í”„ë ˆì„ì—ì„œ ì–¼êµ´ ê²€ì¶œ ì‹œë„)\n",
    "    \"\"\"\n",
    "    input_root_path = Path(input_root)\n",
    "    output_root_path = Path(output_root)\n",
    "    \n",
    "    # í´ë” êµ¬ì¡° ìƒì„±\n",
    "    for label_name in [\"real\", \"fake\"]:\n",
    "        (output_root_path / label_name).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"--- ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘ (Input: {input_root} -> Output: {output_root}) ---\")\n",
    "    \n",
    "    total_files_processed = 0\n",
    "    total_faces_saved = 0\n",
    "\n",
    "    for label_name in [\"real\", \"fake\"]:\n",
    "        input_dir = input_root_path / label_name\n",
    "        output_dir = output_root_path / label_name\n",
    "        \n",
    "        if not input_dir.is_dir():\n",
    "            print(f\"ê²½ê³ : ì…ë ¥ ë¼ë²¨ ë””ë ‰í† ë¦¬ '{input_dir}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n[{label_name} í´ë”] ì²˜ë¦¬ ì¤‘...\")\n",
    "\n",
    "        for file_path in input_dir.iterdir():\n",
    "            ext = file_path.suffix.lower()\n",
    "            file_name_base = file_path.stem\n",
    "            \n",
    "            total_files_processed += 1\n",
    "            faces_in_file = 0\n",
    "            \n",
    "            try:\n",
    "                if ext in IMAGE_EXTS:\n",
    "                    # 1. ì´ë¯¸ì§€ íŒŒì¼ ì²˜ë¦¬ (FaceData.__getitem__ì˜ ë¡œì§ ì°¨ìš©)\n",
    "                    image = Image.open(file_path).convert('RGB')\n",
    "                    face_img = detect_and_crop_face_optimized(image)\n",
    "                    \n",
    "                    if face_img:\n",
    "                        output_path = output_dir / f\"{file_name_base}.{TARGET_IMAGE_FORMAT}\"\n",
    "                        face_img.save(output_path)\n",
    "                        faces_in_file += 1\n",
    "\n",
    "                elif ext in VIDEO_EXTS:\n",
    "                    # 2. ë¹„ë””ì˜¤ íŒŒì¼ ì²˜ë¦¬ (FaceData.__init__ ë° __getitem__ ë¡œì§ ì°¨ìš©)\n",
    "                    cap = cv2.VideoCapture(str(file_path))\n",
    "                    if not cap.isOpened():\n",
    "                        print(f\"  ê²½ê³ : ë¹„ë””ì˜¤ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}\")\n",
    "                        continue\n",
    "                        \n",
    "                    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "                    \n",
    "                    if total_frames > 0:\n",
    "                        frame_indices = []\n",
    "                        if center_frame_only:\n",
    "                            # FaceData í´ë˜ìŠ¤ì˜ ìˆ˜ì •ëœ __init__ ë¡œì§\n",
    "                            frame_indices.append(total_frames // 2) \n",
    "                        else:\n",
    "                            # FaceData í´ë˜ìŠ¤ì˜ ê¸°ì¡´ __init__ ë¡œì§ (ì¼ì •í•œ ê°„ê²© ì¶”ì¶œ)\n",
    "                            frame_indices = np.linspace(\n",
    "                                0, total_frames - 1, 30, dtype=int\n",
    "                            ) \n",
    "                        \n",
    "                        for i, idx in enumerate(frame_indices):\n",
    "                            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "                            ret, frame = cap.read()\n",
    "                            \n",
    "                            if not ret: continue\n",
    "                                \n",
    "                            # BGR (cv2) -> RGB (PIL) ë³€í™˜\n",
    "                            image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "                            face_img = detect_and_crop_face_optimized(image)\n",
    "                            \n",
    "                            if face_img:\n",
    "                                # ë¹„ë””ì˜¤ëŠ” í”„ë ˆì„ ì¸ë±ìŠ¤ë¥¼ íŒŒì¼ëª…ì— ì¶”ê°€í•˜ì—¬ ì €ì¥\n",
    "                                output_path = output_dir / f\"{file_name_base}_frame_{i:04d}.{TARGET_IMAGE_FORMAT}\"\n",
    "                                face_img.save(output_path)\n",
    "                                faces_in_file += 1\n",
    "                                \n",
    "                    cap.release()\n",
    "                \n",
    "                if faces_in_file > 0:\n",
    "                    total_faces_saved += faces_in_file\n",
    "                    print(f\"  âœ… {file_path.name} ì²˜ë¦¬ ì™„ë£Œ. {faces_in_file}ê°œ ì €ì¥.\")\n",
    "                # else:\n",
    "                #     print(f\"  âŒ {file_path.name}ì—ì„œ ì–¼êµ´ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  âŒ ì˜¤ë¥˜: íŒŒì¼ {file_path} ì²˜ë¦¬ ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\")\n",
    "\n",
    "    print(\"\\n--- ì „ì²˜ë¦¬ ìš”ì•½ ---\")\n",
    "    print(f\"ì´ ìŠ¤ìº” íŒŒì¼ ìˆ˜: {total_files_processed}\")\n",
    "    print(f\"ì´ ì €ì¥ëœ ì–¼êµ´ ì´ë¯¸ì§€ ìˆ˜: {total_faces_saved}\")\n",
    "    print(f\"ì €ì¥ í´ë”: {output_root}\")\n",
    "\n",
    "### ë°ì´í„°ë¥¼ ì €ì¥í•  ê²½ìš° í•´ë‹¹ í•¨ìˆ˜ ì‹¤í–‰\n",
    "# preprocess_and_save_dataset(input_root='data', output_root='preprocessed_data', center_frame_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b49211",
   "metadata": {},
   "source": [
    "### Dataset í´ë˜ìŠ¤\n",
    "- train setì—ì„œ ë¶ˆëŸ¬ì˜¤ëŠ” í˜•íƒœ(ì´ë¯¸ì§€ë¥¼ batch ë‹¨ìœ„ë¡œ ë°˜í™˜ ê°€ëŠ¥)\n",
    "- transformì´ Noneì´ë©´, ì´ë¯¸ ì „ì²˜ë¦¬ê°€ ì™„ë£Œëœ ë°ì´í„°ë¥¼ ë°›ì•„ì„œ ì²˜ë¦¬\n",
    "- transformì´ Noneì´ ì•„ë‹ˆë©´, ì›ë³¸ ë°ì´í„°ë¥¼ ë°›ì•„ì„œ transformì— ì „ë‹¬ëœ ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ í†µí•´ì„œ ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf40474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° íŒŒì¼ì€ ì²˜ë¦¬ì˜ ìš©ì´ì„±ì„ ìœ„í•´ real/fakeë¡œ í•˜ìœ„ í´ë”ë¥¼ ë§Œë“¤ì–´ì„œ ì €ì¥\n",
    "\n",
    "### Dataset ê¸°ë°˜ì˜ í´ë˜ìŠ¤\n",
    "\n",
    "class FaceData(Dataset):\n",
    "    \"\"\"\n",
    "    ì–¼êµ´ ì´ë¯¸ì§€/ë¹„ë””ì˜¤ ë°ì´í„°ì…‹ í´ë˜ìŠ¤.\n",
    "    \n",
    "    root ë””ë ‰í† ë¦¬ êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤:\n",
    "    root/\n",
    "        real/\n",
    "            ... (ì´ë¯¸ì§€ ë° ë¹„ë””ì˜¤ íŒŒì¼)\n",
    "        fake/\n",
    "            ... (ì´ë¯¸ì§€ ë° ë¹„ë””ì˜¤ íŒŒì¼)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            root=\"data\",\n",
    "            num_frames_to_extract=30, # ë¹„ë””ì˜¤ì—ì„œ ì¶”ì¶œí•  í”„ë ˆì„ ìˆ˜\n",
    "            transform=None,\n",
    "            is_preprocessed=False,\n",
    "    ):\n",
    "        self.root = Path(root)\n",
    "        self.transform = transform\n",
    "        self.num_frames_to_extract = num_frames_to_extract\n",
    "        self.is_preprocessed = is_preprocessed\n",
    "        \n",
    "        # self.samplesëŠ” (file_path, frame_index, label) íŠœí”Œì„ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "        # frame_indexê°€ Noneì´ë©´ ì´ë¯¸ì§€ íŒŒì¼ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
    "        self.samples = [] \n",
    "        \n",
    "        # í´ë” ì´ë¦„(real/fake)ì— ë”°ë¼ ë¼ë²¨ë§ (real=1, fake=0)\n",
    "        label_map = {\"real\": 0, \"fake\": 1}\n",
    "\n",
    "        if not self.root.is_dir():\n",
    "            raise FileNotFoundError(f\"ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {root}\")\n",
    "\n",
    "        print(\"ë°ì´í„°ì…‹ ìƒ˜í”Œì„ ìŠ¤ìº” ì¤‘ì…ë‹ˆë‹¤...\")\n",
    "        for label_name, label_id in label_map.items():\n",
    "            data_dir = self.root / label_name\n",
    "            if not data_dir.is_dir():\n",
    "                print(f\"ê²½ê³ : ë¼ë²¨ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_dir}\")\n",
    "                continue\n",
    "                \n",
    "            for file_path in data_dir.iterdir():\n",
    "                ext = file_path.suffix.lower()\n",
    "                \n",
    "                try:\n",
    "                    if ext in IMAGE_EXTS:\n",
    "                        # ì´ë¯¸ì§€ íŒŒì¼: (íŒŒì¼ê²½ë¡œ, None, ë¼ë²¨)\n",
    "                        self.samples.append((file_path, None, label_id))\n",
    "                        \n",
    "                    elif ext in VIDEO_EXTS:\n",
    "                        # ë¹„ë””ì˜¤ íŒŒì¼: í”„ë ˆì„ ì¸ë±ìŠ¤ë³„ë¡œ ìƒ˜í”Œ ì¶”ê°€\n",
    "                        cap = cv2.VideoCapture(str(file_path))\n",
    "                        if not cap.isOpened():\n",
    "                            print(f\"ê²½ê³ : ë¹„ë””ì˜¤ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}\")\n",
    "                            continue\n",
    "                            \n",
    "                        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "                        \n",
    "                        # total_framesê°€ 0ë³´ë‹¤ í° ê²½ìš°ì—ë§Œ ì²˜ë¦¬\n",
    "                        if total_frames > 0:\n",
    "                            \n",
    "                            # ğŸ‘‡ **[ìˆ˜ì •ëœ ë¶€ë¶„]** ğŸ‘‡\n",
    "                            # 1. ì¤‘ì•™ í”„ë ˆì„ ì¸ë±ìŠ¤ ê³„ì‚°\n",
    "                            # total_framesëŠ” 1ë¶€í„° ì‹œì‘í•˜ëŠ” ê°œìˆ˜, ì¸ë±ìŠ¤ëŠ” 0ë¶€í„° ì‹œì‘\n",
    "                            center_frame_index = total_frames // 2\n",
    "                            \n",
    "                            # 2. ì¤‘ì•™ í”„ë ˆì„ í•˜ë‚˜ë§Œ self.samplesì— ì¶”ê°€\n",
    "                            self.samples.append((file_path, center_frame_index, label_id))\n",
    "                            # ğŸ‘† **[ìˆ˜ì •ëœ ë¶€ë¶„]** ğŸ‘†\n",
    "\n",
    "                        cap.release()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"ì˜¤ë¥˜: íŒŒì¼ {file_path} ì²˜ë¦¬ ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\")\n",
    "                            \n",
    "        if not self.samples:\n",
    "            print(f\"ê²½ê³ : {root} ë””ë ‰í† ë¦¬ì—ì„œ ì²˜ë¦¬í•  ìƒ˜í”Œì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
    "        else:\n",
    "            print(f\"ì´ {len(self.samples)}ê°œì˜ ìƒ˜í”Œì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"ë°ì´í„°ì…‹ì˜ ì´ ìƒ˜í”Œ ìˆ˜ (ì´ ì–¼êµ´ ì´ë¯¸ì§€ ìˆ˜)ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ë‹¨ì¼ ìƒ˜í”Œ(ì–¼êµ´ ì´ë¯¸ì§€ì™€ ë¼ë²¨)ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "        \"\"\"\n",
    "        file_path, frame_index, label = self.samples[index]\n",
    "        \n",
    "        face_img = None\n",
    "        \n",
    "        try:\n",
    "            if frame_index is None: # Case 1: ì´ë¯¸ì§€ íŒŒì¼\n",
    "                # Image.openì€ RGB ìˆœì„œë¡œ ì´ë¯¸ì§€ë¥¼ ì—½ë‹ˆë‹¤.\n",
    "                image = Image.open(file_path).convert('RGB')\n",
    "                if self.is_preprocessed is False :\n",
    "                    face_img = detect_and_crop_face_optimized(image)\n",
    "                else:\n",
    "                    face_img = image\n",
    "            \n",
    "            else: # Case 2: ë¹„ë””ì˜¤ í”„ë ˆì„\n",
    "                # cv2.VideoCaptureëŠ” ìŠ¤ë ˆë“œ ì•ˆì „í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ,\n",
    "                # num_workers > 0 ì¼ ë•Œë¥¼ ëŒ€ë¹„í•´ __getitem__ ë‚´ì—ì„œ ë§¤ë²ˆ ì—½ë‹ˆë‹¤.\n",
    "                cap = cv2.VideoCapture(str(file_path))\n",
    "                if not cap.isOpened():\n",
    "                     raise IOError(f\"ë¹„ë””ì˜¤ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}\")\n",
    "                \n",
    "                # ì§€ì •ëœ í”„ë ˆì„ ì¸ë±ìŠ¤ë¡œ ì´ë™\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)\n",
    "                ret, frame = cap.read() # frameì€ BGR ìˆœì„œ\n",
    "                cap.release()\n",
    "                \n",
    "                if not ret:\n",
    "                    raise IOError(f\"í”„ë ˆì„ {frame_index}ì„ ì½ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {file_path}\")\n",
    "                    \n",
    "                # BGR (cv2) -> RGB (PIL) ë³€í™˜\n",
    "                image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "                if self.is_preprocessed is False:\n",
    "                    face_img = detect_and_crop_face_optimized(image)\n",
    "                else:\n",
    "                    face_img = image\n",
    "\n",
    "            # ì–¼êµ´ ê²€ì¶œ ì‹¤íŒ¨ ì‹œ (face_imgê°€ Noneì´ê±°ë‚˜ ë¹„ì–´ìˆì„ ê²½ìš°)\n",
    "            if face_img is None:\n",
    "                # print(f\"ê²½ê³ : {file_path} (í”„ë ˆì„: {frame_index})ì—ì„œ ì–¼êµ´ ê²€ì¶œ ì‹¤íŒ¨. ê²€ì€ìƒ‰ ì´ë¯¸ì§€ ë°˜í™˜.\")\n",
    "                # ê²€ì€ìƒ‰ ë¹ˆ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ì—¬ í•™ìŠµì€ ê°€ëŠ¥í•˜ë„ë¡ í•¨\n",
    "                face_img = Image.new('RGB', (224, 224), (0, 0, 0)) \n",
    "                \n",
    "            # ê³µí†µ ì „ì²˜ë¦¬ (Transform) ì ìš© (e.g., ToTensor, Normalize)\n",
    "            if self.transform:\n",
    "                face_img = self.transform(face_img)\n",
    "                \n",
    "            return face_img, label\n",
    "        \n",
    "        except Exception as e:\n",
    "            # ë°ì´í„° ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ, DataLoaderê°€ ë©ˆì¶”ëŠ” ê²ƒì„ ë°©ì§€\n",
    "            print(f\"ì˜¤ë¥˜: ì¸ë±ìŠ¤ {index} ({file_path}, í”„ë ˆì„ {frame_index}) ë¡œë”© ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\")\n",
    "            \n",
    "            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ëŒ€ì²´í•  ë”ë¯¸ ë°ì´í„° ë°˜í™˜\n",
    "            dummy_img = Image.new('RGB', (224, 224), (0, 0, 0)) # 224x224 í¬ê¸° ê°€ì •\n",
    "            if self.transform:\n",
    "                dummy_img = self.transform(dummy_img)\n",
    "            \n",
    "            # -1ê³¼ ê°™ì€ íŠ¹ìˆ˜ ë¼ë²¨ì„ ë°˜í™˜í•˜ì—¬ ë‚˜ì¤‘ì— ë°°ì¹˜ì—ì„œ ì œì™¸í•  ìˆ˜ ìˆë„ë¡ í•¨\n",
    "            return dummy_img, -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3121a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "NORMALIZE_MEAN = [0.485, 0.456, 0.406]\n",
    "NORMALIZE_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=NORMALIZE_MEAN, std=NORMALIZE_STD),\n",
    "])\n",
    "\n",
    "datasets = FaceData(transform=transform, root=\"data\", is_preprocessed=False)\n",
    "\n",
    "# ì´ë¯¸ í¬ë¡­ëœ ë°ì´í„°ê°€ ì €ì¥ë˜ì–´ ìˆë‹¤ë©´ í•´ë‹¹ í•¨ìˆ˜ ì‹¤í–‰\n",
    "# datasets = FaceData(transform=transform, root=\"preprocessed_data\", is_preprocessed=True)\n",
    "\n",
    "total_size = len(datasets)\n",
    "train_size = int(total_size * 0.8)\n",
    "val_size = total_size - train_size\n",
    "\n",
    "# 2. random_splitìœ¼ë¡œ ë°ì´í„°ì…‹ ë¶„ë¦¬\n",
    "train_dataset, val_dataset = random_split(datasets, [train_size, val_size])\n",
    "\n",
    "# 3. DataLoader ìƒì„±\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False) # Valì€ ì„ì„ í•„ìš” ì—†ìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c63466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìŠ¤ì¼€ì¤„ëŸ¬ì— ê´€í•œ í•˜ì´í¼ íŒŒë¼ë¯¸í„°\n",
    "STEP_SIZE=10\n",
    "GAMMA=0.1\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµì— ê´€í•œ í•˜ì´í¼ íŒŒë¼ë¯¸í„°\n",
    "NUM_EPOCHS=20\n",
    "NUM_CLASSES=2\n",
    "UNFREEZE_EPOCH=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283a768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepclfwithResNet(num_classes=NUM_CLASSES).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132fb9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.rgb_branch.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "trainable_params = filter(lambda p : p.requires_grad, model.parameters())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(trainable_params, lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
    "\n",
    "num_epochs = NUM_EPOCHS\n",
    "is_unforzen = False\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    if not is_unfrozen and epoch == UNFREEZE_EPOCH:\n",
    "        print(f\"\\n--- Epoch {epoch + 1}: Unfreezing 'rgb_branch' parameters! ---\")\n",
    "        \n",
    "        for param in model.rgb_branch.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "        optimizer.add_param_group({\n",
    "            'params': model.rgb_branch.parameters(), \n",
    "            'lr': 0.0001 # ì˜ˆ: ê¸°ì¡´ LR(0.001)ë³´ë‹¤ ë‚®ì€ ê°’\n",
    "        })\n",
    "        \n",
    "        is_unfrozen = True # í”Œë˜ê·¸ ì—…ë°ì´íŠ¸\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % 50 == 49:\n",
    "            print(f\"[Epoch {epoch + 1}, Batch {i + 1}] Loss: {running_loss / 100:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "    model.eval() # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì • (Dropout, BatchNorm ë¹„í™œì„±í™”)\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad(): # ê¸°ìš¸ê¸° ê³„ì‚°ì„ ì¤‘ì§€í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½ ë° ì†ë„ í–¥ìƒ\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "\n",
    "            # ëª¨ë¸ ì˜ˆì¸¡\n",
    "            val_outputs = model(val_inputs)\n",
    "            \n",
    "            # ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡ê°’ìœ¼ë¡œ ì„ íƒ\n",
    "            # val_outputs shape: (Batch_Size, Num_Classes)\n",
    "            preds = torch.argmax(val_outputs, dim=1) \n",
    "            \n",
    "            # sklearn ê³„ì‚°ì„ ìœ„í•´ CPUë¡œ ì´ë™ í›„ numpy ë°°ì—´ë¡œ ë³€í™˜\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(val_labels.cpu().numpy())\n",
    "\n",
    "    # --- 3. Macro F1 ìŠ¤ì½”ì–´ ê³„ì‚° ---\n",
    "    # ìˆ˜ì§‘ëœ ëª¨ë“  ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œ ë¼ë²¨ì„ ì‚¬ìš©í•˜ì—¬ F1 ìŠ¤ì½”ì–´ ê³„ì‚°\n",
    "    # zero_division=0 : íŠ¹ì • í´ë˜ìŠ¤ì— ëŒ€í•œ ì˜ˆì¸¡ì´ ì—†ì–´ ë¶„ëª¨ê°€ 0ì´ ë  ê²½ìš° 0.0ìœ¼ë¡œ ì²˜ë¦¬\n",
    "    macro_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    \n",
    "    print(f\"--- [Epoch {epoch + 1}] Validation Macro F1 Score: {macro_f1:.4f} ---\")\n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "    # --- 4. ëª¨ë¸ ì €ì¥ ---\n",
    "    # (ê¸°ì¡´ ì½”ë“œ)\n",
    "    if epoch % 5 == 4:\n",
    "        torch.save(model.state_dict(), f\"CNN_Deepfake_Classifier_{epoch}.pth\")\n",
    "\n",
    "print(\"Finish Training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
